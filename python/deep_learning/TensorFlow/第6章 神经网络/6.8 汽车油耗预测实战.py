import tensorflow as tf
import pandas as pd
from keras import utils, Model, layers, optimizers, losses


# å¯¼å…¥æ•°æ®
def load_data(fname, path_url, column_names):
    """å¯¼å…¥æ•°æ®

    è¿”å›pandas.DataFrameç±»å‹çš„æ•°æ®
    """

    # - é‡‡ç”¨ Auto MPG æ•°æ®é›†ï¼Œå®ƒè®°å½•äº†å„ç§æ±½è½¦æ•ˆèƒ½æŒ‡æ ‡ä¸æ°”ç¼¸æ•°ã€é‡é‡ã€é©¬åŠ›ç­‰å…¶å®ƒå› å­çš„çœŸå®æ•°æ®
    # - Auto MPG æ•°æ®é›†ä¸€å…±è®°å½•äº† 398 é¡¹æ•°æ®ï¼Œæˆ‘ä»¬ä» UCI æœåŠ¡å™¨ä¸‹è½½å¹¶è¯»å–æ•°æ®é›†åˆ°
    dataset_path = utils.get_file(fname, path_url)

    raw_dataset = pd.read_csv(
        dataset_path,
        names=column_names,
        na_values="?",
        comment="\t",
        sep=" ",
        skipinitialspace=True,
    )
    return raw_dataset


# æ•°æ®é¢„å¤„ç†
def preprocess(dataset, batch_size=32):
    """æ•°æ®é¢„å¤„ç†

    input: dataset = pandas.DataFrameå¯¹è±¡
    """
    dataset = dataset.dropna()

    # - ç”±äº Origin å­—æ®µä¸ºç±»åˆ«ç±»å‹æ•°æ®ï¼Œæˆ‘ä»¬å°†å…¶ç§»é™¤ï¼Œå¹¶è½¬æ¢ä¸ºæ–°çš„ 3 ä¸ªå­—æ®µï¼šUSAã€Europe å’Œ Japanï¼Œåˆ†åˆ«ä»£è¡¨æ˜¯å¦æ¥è‡ªæ­¤äº§åœ°
    # å¤„ç†ç±»åˆ«å‹æ•°æ®ï¼Œå…¶ä¸­ origin åˆ—ä»£è¡¨äº†ç±»åˆ« 1,2,3,åˆ†å¸ƒä»£è¡¨äº§åœ°ï¼šç¾å›½ã€æ¬§æ´²ã€æ—¥æœ¬
    # å…ˆå¼¹å‡º(åˆ é™¤å¹¶è¿”å›)origin è¿™ä¸€åˆ—
    origin = dataset.pop("Origin")
    # æ ¹æ® origin åˆ—æ¥å†™å…¥æ–°çš„ 3 ä¸ªåˆ—
    dataset["USA"] = (origin == 1) * 1.0
    dataset["Europe"] = (origin == 2) * 1.0
    dataset["Japan"] = (origin == 3) * 1.0

    # - æŒ‰ç€ 8:2 çš„æ¯”ä¾‹åˆ‡åˆ†æ•°æ®é›†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_dataset = dataset.sample(frac=0.8, random_state=0)
    test_dataset = dataset.drop(train_dataset.index)

    # ç§»åŠ¨ MPG æ²¹è€—æ•ˆèƒ½è¿™ä¸€åˆ—ä¸ºçœŸå®æ ‡ç­¾ Y
    train_labels = train_dataset.pop("MPG")
    test_labels = test_dataset.pop("MPG")

    # - ç»Ÿè®¡è®­ç»ƒé›†çš„å„ä¸ªå­—æ®µæ•°å€¼çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼Œå¹¶å®Œæˆæ•°æ®çš„æ ‡å‡†åŒ–ï¼Œé€šè¿‡ norm()å‡½æ•°å®ç°
    train_stats = train_dataset.describe().transpose()

    # æ ‡å‡†åŒ–æ•°æ®, å‡å»æ¯ä¸ªå­—æ®µçš„å‡å€¼ï¼Œå¹¶é™¤ä»¥æ ‡å‡†å·®
    def norm(x, stats):
        return (x - stats["mean"]) / stats["std"]

    # æ ‡å‡†åŒ–è®­ç»ƒé›†
    # normed_train_data.shape= (314, 9)
    normed_train_data = norm(train_dataset, train_stats)
    # æ ‡å‡†åŒ–æµ‹è¯•é›†
    normed_test_data = norm(test_dataset, train_stats)

    # - åˆ©ç”¨åˆ‡åˆ†çš„è®­ç»ƒé›†æ•°æ®æ„å»ºæ•°æ®é›†å¯¹è±¡
    train_db = tf.data.Dataset.from_tensor_slices(
        (normed_train_data.values, train_labels.values)
    )
    # éšæœºæ‰“æ•£ï¼Œæ‰¹é‡åŒ–
    train_db = train_db.shuffle(100).batch(batch_size)
    return train_db, (normed_test_data, test_labels)


# è®­ç»ƒ
def train(train_db, epoch=20):
    # 3. è®­ç»ƒ
    # åˆ›å»ºç½‘ç»œç±»å®ä¾‹
    model = Network()
    # é€šè¿‡ build å‡½æ•°å®Œæˆå†…éƒ¨å¼ é‡çš„åˆ›å»ºï¼Œå…¶ä¸­ 4 ä¸ºä»»æ„è®¾ç½®çš„ batch æ•°é‡ï¼Œ9 ä¸ºè¾“å…¥ç‰¹å¾é•¿åº¦
    model.build(input_shape=(None, 9))
    model.summary()
    # åŠ é€Ÿæ¢¯åº¦ä¸‹é™
    optimizer = optimizers.RMSprop(learning_rate=0.001)

    for epoch in range(epoch):
        loss = 0.0
        # éå†ä¸€æ¬¡è®­ç»ƒé›†
        for step, (x, y) in enumerate(train_db):
            # æ¢¯åº¦è®°å½•å™¨ï¼Œè®­ç»ƒæ—¶éœ€è¦ä½¿ç”¨å®ƒ
            with tf.GradientTape() as tape:
                # é€šè¿‡ç½‘ç»œè·å¾—è¾“å‡º
                out = model(x)
                # è®¡ç®— MSE
                loss = tf.reduce_mean(losses.MSE(y, out))
                mae_loss = tf.reduce_mean(losses.MAE(y, out))
        # é—´éš”æ€§åœ°æ‰“å°è®­ç»ƒè¯¯å·®
        if epoch % 10 == 0:
            print(epoch, float(loss))
            print(
                f"-epoch={epoch}, MSE_LSS={float(loss)}, MAE_LSS={float(mae_loss)}")
        # è®¡ç®—æ¢¯åº¦ï¼Œå¹¶æ›´æ–°
        grads = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(grads, model.trainable_variables))


# ç½‘ç»œæ¨¡å‹ç±»
class Network(Model):
    # region
    # 6.8.2 åˆ›å»ºç½‘ç»œ
    # è€ƒè™‘åˆ° Auto MPG æ•°æ®é›†è§„æ¨¡è¾ƒå°
    # 1. æˆ‘ä»¬åªåˆ›å»ºä¸€ä¸ª 3 å±‚çš„å…¨è¿æ¥ç½‘ç»œæ¥å®Œæˆ MPGå€¼çš„é¢„æµ‹ä»»åŠ¡ã€‚
    # 1. è¾“å…¥ğ‘¿çš„ç‰¹å¾å…±æœ‰ 9 ç§ï¼Œå› æ­¤ç¬¬ä¸€å±‚çš„è¾“å…¥èŠ‚ç‚¹æ•°ä¸º 9ã€‚
    # 1. ç¬¬ä¸€å±‚ã€ç¬¬äºŒå±‚çš„è¾“å‡ºèŠ‚ç‚¹æ•°è®¾è®¡ä¸º64å’Œ64ï¼Œ
    # 1. ç”±äºåªæœ‰ä¸€ç§é¢„æµ‹å€¼ï¼Œè¾“å‡ºå±‚è¾“å‡ºèŠ‚ç‚¹è®¾è®¡ä¸º 1ã€‚

    # **æ­¥éª¤**

    # 1. æˆ‘ä»¬å°†ç½‘ç»œå®ç°ä¸ºä¸€ä¸ªè‡ªå®šä¹‰ç½‘ç»œç±»
    # 1. åªéœ€è¦åœ¨åˆå§‹åŒ–å‡½æ•°ä¸­åˆ›å»ºå„ä¸ªå­ç½‘ç»œå±‚ï¼Œ
    # 1. å¹¶åœ¨å‰å‘è®¡ç®—å‡½æ•° call ä¸­å®ç°è‡ªå®šä¹‰ç½‘ç»œç±»çš„è®¡ç®—é€»è¾‘å³å¯ã€‚
    # 1. è‡ªå®šä¹‰ç½‘ç»œç±»ç»§æ‰¿è‡ªkeras.Model åŸºç±»ï¼Œè¿™ä¹Ÿæ˜¯è‡ªå®šä¹‰ç½‘ç»œç±»çš„æ ‡å‡†å†™æ³•ï¼Œä»¥æ–¹ä¾¿åœ°åˆ©ç”¨ keras.Model åŸºç±»æä¾›çš„ trainable_variablesã€save_weights ç­‰å„ç§ä¾¿æ·åŠŸèƒ½ã€‚
    # endregion

    # å›å½’ç½‘ç»œæ¨¡å‹
    def __init__(self):
        super(Network, self).__init__()
        # åˆ›å»º 3 ä¸ªå…¨è¿æ¥å±‚
        # self.fc1 = layers.Dense(1)
        self.fc1 = layers.Dense(32, activation="relu")
        self.fc2 = layers.Dense(64, activation="relu")
        self.fc3 = layers.Dense(128, activation="relu")
        self.fc4 = layers.Dense(64, activation="relu")
        self.fc5 = layers.Dense(32, activation="relu")
        self.fc6 = layers.Dense(1)

    def call(self, inputs, training=None, mask=None):
        # ä¾æ¬¡é€šè¿‡ä¸‰ä¸ªå…¨è¿æ¥å±‚
        x = self.fc1(inputs)
        x = self.fc2(x)
        x = self.fc3(x)
        x = self.fc4(x)
        x = self.fc5(x)
        x = self.fc6(x)
        return x


# #### 6.8 æ±½è½¦æ²¹è€—é¢„æµ‹å®æˆ˜
if __name__ == "__main__":
    # #### 6.8 æ±½è½¦æ²¹è€—é¢„æµ‹å®æˆ˜
    # æœ¬èŠ‚æˆ‘ä»¬å°†åˆ©ç”¨å…¨è¿æ¥ç½‘ç»œæ¨¡å‹æ¥å®Œæˆæ±½è½¦çš„æ•ˆèƒ½æŒ‡æ ‡ MPG(Mile Per Gallonï¼Œæ¯åŠ ä»‘ç‡ƒæ²¹è‹±é‡Œæ•°)çš„é¢„æµ‹é—®é¢˜å®æˆ˜

    # ###### 6.8.1 æ•°æ®é›†
    # region
    fname = "auto-mpg.data"
    path_url = "http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"
    # æ¯ä¸€åˆ—çš„æ„ä¹‰
    # MPG - æ¯åŠ ä»‘ç‡ƒæ²¹è‹±é‡Œ
    # Cylinders - æ°”ç¼¸æ•°
    # Displacement - æ’é‡
    # Horsepower - é©¬åŠ›
    # Weight - é‡é‡
    # Acceleration - åŠ é€Ÿåº¦
    # Model - å‹å·
    # Year - å¹´ä»½
    # Origin - äº§åœ°
    # endregion
    column_names = [
        "MPG",
        "Cylinders",
        "Displacement",
        "Horsepower",
        "Weight",
        "Acceleration",
        "Model Year",
        "Origin",
    ]
    # 1. å¯¼å…¥æ•°æ®
    dataset = load_data(fname, path_url, column_names)
    # 2. æ•°æ®é¢„å¤„ç†
    train_db, (test_data, test_labels) = preprocess(dataset, batch_size=128)

    # 3. è®­ç»ƒ
    train(train_db, epoch=400)

    pass
