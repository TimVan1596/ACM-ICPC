{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic 泰坦尼克号预测\n",
    "\n",
    "基本思路为:\n",
    "1. 导入数据集\n",
    "2. 对数据预处理\n",
    "3. 训练\n",
    "4. 预测并输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提前将所有包引入\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import activations\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import regularizers\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.导入数据 load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据\n",
    "def load_data(path_url, test_path_url):\n",
    "    raw_train_dataset = pd.read_csv(path_url)\n",
    "    raw_test_dataset = pd.read_csv(test_path_url)\n",
    "    return raw_train_dataset, raw_test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_train_dataset.shape=(891, 12)\n",
      "raw_test_dataset.shape=(418, 11)\n"
     ]
    }
   ],
   "source": [
    "# 1. 导入数据\n",
    "path_url = r\"kaggle\\titanic\\train.csv\"\n",
    "test_path_url = r\"kaggle\\titanic\\test.csv\"\n",
    "raw_train_dataset, raw_test_dataset = load_data(path_url, test_path_url)\n",
    "print(f\"raw_train_dataset.shape={raw_train_dataset.shape}\")\n",
    "print(f\"raw_test_dataset.shape={raw_test_dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.数据预处理 preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的基础方法\n",
    "def preprocess(raw_dataset, features, train=True):\n",
    "    \"\"\"用于predict的数据预处理\n",
    "    Args:\n",
    "        input: dataset = pandas.DataFrame对象\n",
    "    \"\"\"\n",
    "    # 以中位数来替代\n",
    "    if \"Age\" in features:\n",
    "        raw_dataset[\"Age\"].fillna(raw_dataset[\"Age\"].median(), inplace=True)\n",
    "    raw_dataset[\"Fare\"].fillna(raw_dataset[\"Fare\"].median(), inplace=True)\n",
    "    raw_dataset[\"Embarked\"].fillna(raw_dataset[\"Fare\"].median(), inplace=True)\n",
    "    dataset = raw_dataset[features]\n",
    "    dataset = dataset.copy()\n",
    "\n",
    "    # 由于 embarked=登船港口, Port of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton\n",
    "\n",
    "    Embarked = dataset.pop(\"Embarked\")\n",
    "\n",
    "    # 根据 embarked 列来写入新的 3 个列\n",
    "    dataset[\"S\"] = (Embarked == \"S\") * 1.0\n",
    "    dataset[\"C\"] = (Embarked == \"C\") * 1.0\n",
    "    dataset[\"Q\"] = (Embarked == \"Q\") * 1.0\n",
    "\n",
    "    # 根据 sex 列来写入新的 2 个列\n",
    "    Sex = dataset.pop(\"Sex\")\n",
    "    dataset[\"Male\"] = (Sex == \"male\") * 1.0\n",
    "    dataset[\"Female\"] = (Sex == \"female\") * 1.0\n",
    "\n",
    "    dataset_withoutna = dataset\n",
    "    if train:\n",
    "        labels = dataset_withoutna[\"Survived\"]\n",
    "        dataset_withoutna.pop(\"PassengerId\")\n",
    "        dataset_withoutna.pop(\"Survived\")\n",
    "        # 标准化,归一化输入\n",
    "        train_stats = dataset_withoutna.describe()\n",
    "        train_stats = train_stats.transpose()\n",
    "        normed_train_data = (dataset_withoutna - train_stats[\"mean\"]) / train_stats[\n",
    "            \"std\"\n",
    "        ]\n",
    "        return np.array(normed_train_data), np.array(labels)\n",
    "    else:\n",
    "        labels = dataset.pop(\"PassengerId\")\n",
    "        dataset.fillna(0, inplace=True)\n",
    "        test_stats = dataset.describe()\n",
    "        test_stats = test_stats.transpose()\n",
    "        normed_test_data = (dataset - test_stats[\"mean\"]) / test_stats[\"std\"]\n",
    "        return np.array(normed_test_data), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset.shape=(891, 10)\n",
      "labels.shape=(891,)\n"
     ]
    }
   ],
   "source": [
    "    # 2. 数据预处理\n",
    "    features_test = [\n",
    "        \"PassengerId\",\n",
    "        \"Pclass\",\n",
    "        \"Sex\",\n",
    "        \"Fare\",\n",
    "        \"Age\",\n",
    "        \"SibSp\",\n",
    "        \"Parch\",\n",
    "        \"Embarked\",\n",
    "    ]\n",
    "    features_train = features_test + [\"Survived\"]\n",
    "\n",
    "    # 获取预处理后的训练集和标签\n",
    "    train_dataset, labels = preprocess(raw_train_dataset, features_train)\n",
    "    print(f\"train_dataset.shape={train_dataset.shape}\")\n",
    "    print(f\"labels.shape={labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.训练 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataset, labels, epochs=120, batch_size=512, is_plot=False):\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # 1. input_shape = 输入形状\n",
    "            # ND 张量的形状：. 最常见的情况是带有 shape 的 2D 输入。\n",
    "            # (batch_size, ..., input_dim)(batch_size, input_dim)\n",
    "            # train_dataset.shape[1] = 4\n",
    "            # 现在模型就会以尺寸为 (*, 4) 的数组作为输入，\n",
    "            # 2. kernel_regularizer = 应用于kernel权重矩阵的正则化函数。\n",
    "            # 𝐿2正则化：范数的平方\n",
    "            tf.keras.layers.Dense(\n",
    "                64,\n",
    "                activation=\"relu\",\n",
    "                input_shape=(train_dataset.shape[1],),\n",
    "                kernel_regularizer=regularizers.l2(0.001),\n",
    "            ),\n",
    "            tf.keras.layers.Dense(\n",
    "                32, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)\n",
    "            ),\n",
    "            tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(1, name=\"prediction\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 在 Keras 中提供了 compile()和 fit()函数方便实现逻辑。\n",
    "    # compile：首先通过compile 函数指定网络使用的优化器对象、损失函数类型，评价指标等设定，这一步称为装配\n",
    "    # fit： 模型装配完成后，即可通过 fit()函数送入待训练的数据集和验证用的数据集\n",
    "    model.compile(\n",
    "        # Adam的学习律默认为0.001\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        # BinaryCrossentropy:计算真实标签和预测标签之间的交叉熵损失\n",
    "        # i.e, value in [-inf, inf] when from_logits=True\n",
    "        # i.e, value in [0., 1.] when from_logits=False\n",
    "        # from_logits=False：\n",
    "        # 当from_logits = True时，网络预测值y_pred必须为还没经过Softmax、sigmoid等函数的变量；\n",
    "        # 当from_logits=False时，网络预测值y_pred是经过概率化后的值。\n",
    "        # 当 from_logits 设置为 True 时，y_pred 表示须为未经过 Softmax 函数的变量 z；\n",
    "        # 当 from_logits 设置为 False 时，y_pred 表示为经过 Softmax 函数的输出。\n",
    "        # 为了数值计算稳定性，一般设置 from_logits 为 True，\n",
    "        # 此时tf.keras.losses.categorical_crossentropy 将在内部进行 Softmax 函数计算，\n",
    "        # 所以不需要在模型中显式调用 Softmax 函数\n",
    "        # 这样 categorical_crossentropy 函数在计算损失函数前，会先内部调用 Softmax 函数\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        # 设置测量指标为准确率\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    # early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    # 模型装配完成后，即可通过 fit()函数送入待训练的数据集和验证用的数据集，这一步称为模型训练\n",
    "    # fit函数的参数:\n",
    "    # x = Input data\n",
    "    # y = Target data\n",
    "    # verbose = 'auto'、0、1 或 2 详细模式。\n",
    "    # 0 = 静音，1 = 进度条，2 = 每个 epoch 一行。'auto' 在大多数情况下默认为 1\n",
    "    history = model.fit(\n",
    "        x=train_dataset,\n",
    "        y=labels,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.01,\n",
    "        batch_size=batch_size,\n",
    "        verbose=\"auto\",\n",
    "        # callbacks=[early_stop]\n",
    "    )\n",
    "\n",
    "    # 显示训练情况\n",
    "    if is_plot:\n",
    "        plot_history(history)\n",
    "\n",
    "    # 可以通过 Model.evaluate(db)循环测试完 db 数据集上所有样本\n",
    "    loss, accuracy = model.evaluate(train_dataset, labels, verbose=2)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显示训练情况 plot_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示训练情况\n",
    "def plot_history(history):\n",
    "    # histoty的返回值\n",
    "    # A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).\n",
    "    # 一个历史对象。它的 History.history 属性是连续 epoch 的训练损失值和指标值的记录，\n",
    "    # 以及验证损失值和验证指标值（如果适用）。\n",
    "    # history.history：loss、accuracy、val loss、val accuracy\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist[\"epoch\"] = history.epoch\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Num of Epochs\")\n",
    "    plt.ylabel(\"value\")\n",
    "    # plt.plot(hist[\"epoch\"], hist[\"loss\"], label=\"Loss\")\n",
    "    # plt.plot(hist[\"epoch\"], hist[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot(hist[\"epoch\"], hist[\"accuracy\"], label=\"accuracy\")\n",
    "    plt.plot(hist[\"epoch\"], hist[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "2/2 [==============================] - 1s 165ms/step - loss: 0.7458 - accuracy: 0.6111 - val_loss: 0.7122 - val_accuracy: 0.7778\n",
      "Epoch 2/256\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.7279 - accuracy: 0.6122 - val_loss: 0.7000 - val_accuracy: 0.7778\n",
      "Epoch 3/256\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.7108 - accuracy: 0.6134 - val_loss: 0.6884 - val_accuracy: 0.7778\n",
      "Epoch 4/256\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6954 - accuracy: 0.6145 - val_loss: 0.6780 - val_accuracy: 0.7778\n",
      "Epoch 5/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.6809 - accuracy: 0.6145 - val_loss: 0.6682 - val_accuracy: 0.7778\n",
      "Epoch 6/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.6664 - accuracy: 0.6156 - val_loss: 0.6591 - val_accuracy: 0.7778\n",
      "Epoch 7/256\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6527 - accuracy: 0.6202 - val_loss: 0.6502 - val_accuracy: 0.8889\n",
      "Epoch 8/256\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.6395 - accuracy: 0.6247 - val_loss: 0.6410 - val_accuracy: 0.8889\n",
      "Epoch 9/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.6265 - accuracy: 0.6633 - val_loss: 0.6318 - val_accuracy: 0.8889\n",
      "Epoch 10/256\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6128 - accuracy: 0.6939 - val_loss: 0.6215 - val_accuracy: 0.8889\n",
      "Epoch 11/256\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5997 - accuracy: 0.7290 - val_loss: 0.6120 - val_accuracy: 0.8889\n",
      "Epoch 12/256\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.5869 - accuracy: 0.7551 - val_loss: 0.6039 - val_accuracy: 0.8889\n",
      "Epoch 13/256\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5749 - accuracy: 0.7721 - val_loss: 0.5969 - val_accuracy: 0.8889\n",
      "Epoch 14/256\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.5626 - accuracy: 0.7812 - val_loss: 0.5880 - val_accuracy: 0.8889\n",
      "Epoch 15/256\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.5509 - accuracy: 0.7982 - val_loss: 0.5800 - val_accuracy: 0.8889\n",
      "Epoch 16/256\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5400 - accuracy: 0.8073 - val_loss: 0.5714 - val_accuracy: 0.7778\n",
      "Epoch 17/256\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5302 - accuracy: 0.8129 - val_loss: 0.5638 - val_accuracy: 0.7778\n",
      "Epoch 18/256\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5218 - accuracy: 0.8163 - val_loss: 0.5563 - val_accuracy: 0.6667\n",
      "Epoch 19/256\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5140 - accuracy: 0.8163 - val_loss: 0.5482 - val_accuracy: 0.6667\n",
      "Epoch 20/256\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5071 - accuracy: 0.8163 - val_loss: 0.5402 - val_accuracy: 0.6667\n",
      "Epoch 21/256\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5016 - accuracy: 0.8152 - val_loss: 0.5300 - val_accuracy: 0.6667\n",
      "Epoch 22/256\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4966 - accuracy: 0.8163 - val_loss: 0.5199 - val_accuracy: 0.6667\n",
      "Epoch 23/256\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4930 - accuracy: 0.8175 - val_loss: 0.5101 - val_accuracy: 0.7778\n",
      "Epoch 24/256\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4896 - accuracy: 0.8163 - val_loss: 0.4985 - val_accuracy: 0.7778\n",
      "Epoch 25/256\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4866 - accuracy: 0.8163 - val_loss: 0.4878 - val_accuracy: 0.7778\n",
      "Epoch 26/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4841 - accuracy: 0.8152 - val_loss: 0.4752 - val_accuracy: 0.7778\n",
      "Epoch 27/256\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4819 - accuracy: 0.8141 - val_loss: 0.4671 - val_accuracy: 0.7778\n",
      "Epoch 28/256\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4798 - accuracy: 0.8175 - val_loss: 0.4606 - val_accuracy: 0.7778\n",
      "Epoch 29/256\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.4775 - accuracy: 0.8197 - val_loss: 0.4577 - val_accuracy: 0.7778\n",
      "Epoch 30/256\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4759 - accuracy: 0.8175 - val_loss: 0.4539 - val_accuracy: 0.8889\n",
      "Epoch 31/256\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4740 - accuracy: 0.8175 - val_loss: 0.4486 - val_accuracy: 0.8889\n",
      "Epoch 32/256\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4723 - accuracy: 0.8186 - val_loss: 0.4444 - val_accuracy: 0.8889\n",
      "Epoch 33/256\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4704 - accuracy: 0.8163 - val_loss: 0.4367 - val_accuracy: 0.8889\n",
      "Epoch 34/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4689 - accuracy: 0.8163 - val_loss: 0.4306 - val_accuracy: 0.8889\n",
      "Epoch 35/256\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4672 - accuracy: 0.8163 - val_loss: 0.4271 - val_accuracy: 0.8889\n",
      "Epoch 36/256\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4656 - accuracy: 0.8175 - val_loss: 0.4252 - val_accuracy: 0.8889\n",
      "Epoch 37/256\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4642 - accuracy: 0.8220 - val_loss: 0.4222 - val_accuracy: 0.8889\n",
      "Epoch 38/256\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4626 - accuracy: 0.8243 - val_loss: 0.4182 - val_accuracy: 0.8889\n",
      "Epoch 39/256\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4612 - accuracy: 0.8277 - val_loss: 0.4167 - val_accuracy: 0.8889\n",
      "Epoch 40/256\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4599 - accuracy: 0.8288 - val_loss: 0.4144 - val_accuracy: 0.8889\n",
      "Epoch 41/256\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4586 - accuracy: 0.8288 - val_loss: 0.4114 - val_accuracy: 0.8889\n",
      "Epoch 42/256\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4573 - accuracy: 0.8277 - val_loss: 0.4099 - val_accuracy: 0.8889\n",
      "Epoch 43/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4562 - accuracy: 0.8288 - val_loss: 0.4098 - val_accuracy: 0.8889\n",
      "Epoch 44/256\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4549 - accuracy: 0.8288 - val_loss: 0.4051 - val_accuracy: 0.8889\n",
      "Epoch 45/256\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4540 - accuracy: 0.8299 - val_loss: 0.3996 - val_accuracy: 0.8889\n",
      "Epoch 46/256\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4529 - accuracy: 0.8299 - val_loss: 0.3907 - val_accuracy: 0.8889\n",
      "Epoch 47/256\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4518 - accuracy: 0.8277 - val_loss: 0.3840 - val_accuracy: 0.8889\n",
      "Epoch 48/256\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4508 - accuracy: 0.8299 - val_loss: 0.3845 - val_accuracy: 0.8889\n",
      "Epoch 49/256\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4499 - accuracy: 0.8299 - val_loss: 0.3837 - val_accuracy: 0.8889\n",
      "Epoch 50/256\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4488 - accuracy: 0.8311 - val_loss: 0.3810 - val_accuracy: 0.8889\n",
      "Epoch 51/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4479 - accuracy: 0.8322 - val_loss: 0.3802 - val_accuracy: 0.8889\n",
      "Epoch 52/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4467 - accuracy: 0.8333 - val_loss: 0.3754 - val_accuracy: 0.8889\n",
      "Epoch 53/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4459 - accuracy: 0.8333 - val_loss: 0.3714 - val_accuracy: 0.8889\n",
      "Epoch 54/256\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4451 - accuracy: 0.8345 - val_loss: 0.3691 - val_accuracy: 0.8889\n",
      "Epoch 55/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4443 - accuracy: 0.8345 - val_loss: 0.3685 - val_accuracy: 0.8889\n",
      "Epoch 56/256\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4436 - accuracy: 0.8356 - val_loss: 0.3654 - val_accuracy: 0.8889\n",
      "Epoch 57/256\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4427 - accuracy: 0.8379 - val_loss: 0.3635 - val_accuracy: 0.8889\n",
      "Epoch 58/256\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4417 - accuracy: 0.8367 - val_loss: 0.3649 - val_accuracy: 0.8889\n",
      "Epoch 59/256\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.4408 - accuracy: 0.8356 - val_loss: 0.3662 - val_accuracy: 0.8889\n",
      "Epoch 60/256\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4401 - accuracy: 0.8379 - val_loss: 0.3659 - val_accuracy: 0.8889\n",
      "Epoch 61/256\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4395 - accuracy: 0.8379 - val_loss: 0.3650 - val_accuracy: 0.8889\n",
      "Epoch 62/256\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4387 - accuracy: 0.8367 - val_loss: 0.3592 - val_accuracy: 0.8889\n",
      "Epoch 63/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4379 - accuracy: 0.8379 - val_loss: 0.3549 - val_accuracy: 0.8889\n",
      "Epoch 64/256\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4372 - accuracy: 0.8390 - val_loss: 0.3529 - val_accuracy: 0.8889\n",
      "Epoch 65/256\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.4364 - accuracy: 0.8390 - val_loss: 0.3517 - val_accuracy: 0.8889\n",
      "Epoch 66/256\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4358 - accuracy: 0.8390 - val_loss: 0.3512 - val_accuracy: 0.8889\n",
      "Epoch 67/256\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4351 - accuracy: 0.8401 - val_loss: 0.3485 - val_accuracy: 0.8889\n",
      "Epoch 68/256\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4344 - accuracy: 0.8401 - val_loss: 0.3458 - val_accuracy: 0.8889\n",
      "Epoch 69/256\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.4336 - accuracy: 0.8401 - val_loss: 0.3449 - val_accuracy: 0.8889\n",
      "Epoch 70/256\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4329 - accuracy: 0.8401 - val_loss: 0.3446 - val_accuracy: 0.8889\n",
      "Epoch 71/256\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4323 - accuracy: 0.8401 - val_loss: 0.3439 - val_accuracy: 0.8889\n",
      "Epoch 72/256\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.4316 - accuracy: 0.8401 - val_loss: 0.3458 - val_accuracy: 0.8889\n",
      "Epoch 73/256\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4309 - accuracy: 0.8413 - val_loss: 0.3466 - val_accuracy: 0.8889\n",
      "Epoch 74/256\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4302 - accuracy: 0.8413 - val_loss: 0.3478 - val_accuracy: 0.8889\n",
      "Epoch 75/256\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4297 - accuracy: 0.8413 - val_loss: 0.3464 - val_accuracy: 0.8889\n",
      "Epoch 76/256\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.4290 - accuracy: 0.8424 - val_loss: 0.3437 - val_accuracy: 0.8889\n",
      "Epoch 77/256\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4284 - accuracy: 0.8413 - val_loss: 0.3385 - val_accuracy: 0.8889\n",
      "Epoch 78/256\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4278 - accuracy: 0.8413 - val_loss: 0.3367 - val_accuracy: 0.8889\n",
      "Epoch 79/256\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.4271 - accuracy: 0.8379 - val_loss: 0.3349 - val_accuracy: 0.8889\n",
      "Epoch 80/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4266 - accuracy: 0.8379 - val_loss: 0.3343 - val_accuracy: 0.8889\n",
      "Epoch 81/256\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4260 - accuracy: 0.8379 - val_loss: 0.3359 - val_accuracy: 0.8889\n",
      "Epoch 82/256\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4254 - accuracy: 0.8379 - val_loss: 0.3371 - val_accuracy: 0.8889\n",
      "Epoch 83/256\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.4247 - accuracy: 0.8413 - val_loss: 0.3385 - val_accuracy: 0.8889\n",
      "Epoch 84/256\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.4242 - accuracy: 0.8424 - val_loss: 0.3398 - val_accuracy: 0.8889\n",
      "Epoch 85/256\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4236 - accuracy: 0.8447 - val_loss: 0.3430 - val_accuracy: 0.8889\n",
      "Epoch 86/256\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4232 - accuracy: 0.8447 - val_loss: 0.3442 - val_accuracy: 0.8889\n",
      "Epoch 87/256\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4226 - accuracy: 0.8447 - val_loss: 0.3426 - val_accuracy: 0.8889\n",
      "Epoch 88/256\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4220 - accuracy: 0.8447 - val_loss: 0.3415 - val_accuracy: 0.8889\n",
      "Epoch 89/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4214 - accuracy: 0.8447 - val_loss: 0.3392 - val_accuracy: 0.8889\n",
      "Epoch 90/256\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4210 - accuracy: 0.8447 - val_loss: 0.3418 - val_accuracy: 0.8889\n",
      "Epoch 91/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4204 - accuracy: 0.8435 - val_loss: 0.3421 - val_accuracy: 0.8889\n",
      "Epoch 92/256\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4199 - accuracy: 0.8435 - val_loss: 0.3423 - val_accuracy: 0.8889\n",
      "Epoch 93/256\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.4193 - accuracy: 0.8435 - val_loss: 0.3443 - val_accuracy: 0.8889\n",
      "Epoch 94/256\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4194 - accuracy: 0.8435 - val_loss: 0.3464 - val_accuracy: 0.8889\n",
      "Epoch 95/256\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4183 - accuracy: 0.8435 - val_loss: 0.3415 - val_accuracy: 0.8889\n",
      "Epoch 96/256\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4178 - accuracy: 0.8435 - val_loss: 0.3398 - val_accuracy: 0.8889\n",
      "Epoch 97/256\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.4172 - accuracy: 0.8435 - val_loss: 0.3402 - val_accuracy: 0.8889\n",
      "Epoch 98/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4167 - accuracy: 0.8435 - val_loss: 0.3402 - val_accuracy: 0.8889\n",
      "Epoch 99/256\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4162 - accuracy: 0.8435 - val_loss: 0.3397 - val_accuracy: 0.8889\n",
      "Epoch 100/256\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.4156 - accuracy: 0.8435 - val_loss: 0.3403 - val_accuracy: 0.8889\n",
      "Epoch 101/256\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4153 - accuracy: 0.8435 - val_loss: 0.3377 - val_accuracy: 0.8889\n",
      "Epoch 102/256\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.4149 - accuracy: 0.8447 - val_loss: 0.3398 - val_accuracy: 0.8889\n",
      "Epoch 103/256\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4141 - accuracy: 0.8435 - val_loss: 0.3363 - val_accuracy: 0.8889\n",
      "Epoch 104/256\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4138 - accuracy: 0.8435 - val_loss: 0.3323 - val_accuracy: 0.8889\n",
      "Epoch 105/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4133 - accuracy: 0.8435 - val_loss: 0.3350 - val_accuracy: 0.8889\n",
      "Epoch 106/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4129 - accuracy: 0.8435 - val_loss: 0.3357 - val_accuracy: 0.8889\n",
      "Epoch 107/256\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4124 - accuracy: 0.8435 - val_loss: 0.3356 - val_accuracy: 0.8889\n",
      "Epoch 108/256\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.4120 - accuracy: 0.8435 - val_loss: 0.3388 - val_accuracy: 0.8889\n",
      "Epoch 109/256\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4114 - accuracy: 0.8435 - val_loss: 0.3396 - val_accuracy: 0.8889\n",
      "Epoch 110/256\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4109 - accuracy: 0.8435 - val_loss: 0.3409 - val_accuracy: 0.8889\n",
      "Epoch 111/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4105 - accuracy: 0.8447 - val_loss: 0.3420 - val_accuracy: 0.8889\n",
      "Epoch 112/256\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.4100 - accuracy: 0.8458 - val_loss: 0.3422 - val_accuracy: 0.8889\n",
      "Epoch 113/256\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4095 - accuracy: 0.8458 - val_loss: 0.3393 - val_accuracy: 0.8889\n",
      "Epoch 114/256\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.4096 - accuracy: 0.8447 - val_loss: 0.3358 - val_accuracy: 0.8889\n",
      "Epoch 115/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4088 - accuracy: 0.8435 - val_loss: 0.3380 - val_accuracy: 0.8889\n",
      "Epoch 116/256\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.4084 - accuracy: 0.8458 - val_loss: 0.3395 - val_accuracy: 0.8889\n",
      "Epoch 117/256\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4079 - accuracy: 0.8458 - val_loss: 0.3402 - val_accuracy: 0.8889\n",
      "Epoch 118/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4075 - accuracy: 0.8447 - val_loss: 0.3389 - val_accuracy: 0.8889\n",
      "Epoch 119/256\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4070 - accuracy: 0.8447 - val_loss: 0.3417 - val_accuracy: 0.8889\n",
      "Epoch 120/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4066 - accuracy: 0.8458 - val_loss: 0.3440 - val_accuracy: 0.8889\n",
      "Epoch 121/256\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.4062 - accuracy: 0.8458 - val_loss: 0.3404 - val_accuracy: 0.8889\n",
      "Epoch 122/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4056 - accuracy: 0.8447 - val_loss: 0.3387 - val_accuracy: 0.8889\n",
      "Epoch 123/256\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.4052 - accuracy: 0.8447 - val_loss: 0.3340 - val_accuracy: 0.8889\n",
      "Epoch 124/256\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.4048 - accuracy: 0.8447 - val_loss: 0.3333 - val_accuracy: 0.8889\n",
      "Epoch 125/256\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4044 - accuracy: 0.8447 - val_loss: 0.3367 - val_accuracy: 0.8889\n",
      "Epoch 126/256\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4038 - accuracy: 0.8447 - val_loss: 0.3369 - val_accuracy: 0.8889\n",
      "Epoch 127/256\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.4034 - accuracy: 0.8447 - val_loss: 0.3390 - val_accuracy: 0.8889\n",
      "Epoch 128/256\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4030 - accuracy: 0.8458 - val_loss: 0.3387 - val_accuracy: 0.8889\n",
      "Epoch 129/256\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.4026 - accuracy: 0.8458 - val_loss: 0.3376 - val_accuracy: 0.8889\n",
      "Epoch 130/256\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.4022 - accuracy: 0.8447 - val_loss: 0.3386 - val_accuracy: 0.8889\n",
      "Epoch 131/256\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4019 - accuracy: 0.8447 - val_loss: 0.3405 - val_accuracy: 0.8889\n",
      "Epoch 132/256\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.4012 - accuracy: 0.8458 - val_loss: 0.3383 - val_accuracy: 0.8889\n",
      "Epoch 133/256\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4008 - accuracy: 0.8447 - val_loss: 0.3347 - val_accuracy: 0.8889\n",
      "Epoch 134/256\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.4005 - accuracy: 0.8447 - val_loss: 0.3354 - val_accuracy: 0.8889\n",
      "Epoch 135/256\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4002 - accuracy: 0.8447 - val_loss: 0.3334 - val_accuracy: 0.8889\n",
      "Epoch 136/256\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3994 - accuracy: 0.8447 - val_loss: 0.3359 - val_accuracy: 0.8889\n",
      "Epoch 137/256\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3990 - accuracy: 0.8447 - val_loss: 0.3366 - val_accuracy: 0.8889\n",
      "Epoch 138/256\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3987 - accuracy: 0.8458 - val_loss: 0.3386 - val_accuracy: 0.8889\n",
      "Epoch 139/256\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3983 - accuracy: 0.8447 - val_loss: 0.3365 - val_accuracy: 0.8889\n",
      "Epoch 140/256\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3978 - accuracy: 0.8447 - val_loss: 0.3380 - val_accuracy: 0.8889\n",
      "Epoch 141/256\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3977 - accuracy: 0.8458 - val_loss: 0.3398 - val_accuracy: 0.8889\n",
      "Epoch 142/256\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.3973 - accuracy: 0.8447 - val_loss: 0.3391 - val_accuracy: 0.8889\n",
      "Epoch 143/256\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3966 - accuracy: 0.8458 - val_loss: 0.3404 - val_accuracy: 0.8889\n",
      "Epoch 144/256\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3963 - accuracy: 0.8458 - val_loss: 0.3427 - val_accuracy: 0.8889\n",
      "Epoch 145/256\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3962 - accuracy: 0.8458 - val_loss: 0.3513 - val_accuracy: 0.8889\n",
      "Epoch 146/256\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3962 - accuracy: 0.8458 - val_loss: 0.3489 - val_accuracy: 0.8889\n",
      "Epoch 147/256\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.3953 - accuracy: 0.8458 - val_loss: 0.3471 - val_accuracy: 0.8889\n",
      "Epoch 148/256\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.3947 - accuracy: 0.8458 - val_loss: 0.3424 - val_accuracy: 0.8889\n",
      "Epoch 149/256\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3944 - accuracy: 0.8447 - val_loss: 0.3389 - val_accuracy: 0.8889\n",
      "Epoch 150/256\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3943 - accuracy: 0.8447 - val_loss: 0.3367 - val_accuracy: 0.8889\n",
      "Epoch 151/256\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3941 - accuracy: 0.8447 - val_loss: 0.3299 - val_accuracy: 0.8889\n",
      "Epoch 152/256\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.3935 - accuracy: 0.8447 - val_loss: 0.3307 - val_accuracy: 0.8889\n",
      "Epoch 153/256\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3933 - accuracy: 0.8458 - val_loss: 0.3394 - val_accuracy: 0.8889\n",
      "Epoch 154/256\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3928 - accuracy: 0.8458 - val_loss: 0.3416 - val_accuracy: 0.8889\n",
      "Epoch 155/256\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3922 - accuracy: 0.8458 - val_loss: 0.3437 - val_accuracy: 0.8889\n",
      "Epoch 156/256\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.3920 - accuracy: 0.8458 - val_loss: 0.3454 - val_accuracy: 0.8889\n",
      "Epoch 157/256\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.3915 - accuracy: 0.8458 - val_loss: 0.3422 - val_accuracy: 0.8889\n",
      "Epoch 158/256\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3912 - accuracy: 0.8458 - val_loss: 0.3356 - val_accuracy: 0.8889\n",
      "Epoch 159/256\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3909 - accuracy: 0.8447 - val_loss: 0.3326 - val_accuracy: 0.8889\n",
      "Epoch 160/256\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3906 - accuracy: 0.8447 - val_loss: 0.3362 - val_accuracy: 0.8889\n",
      "Epoch 161/256\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.3901 - accuracy: 0.8447 - val_loss: 0.3384 - val_accuracy: 0.8889\n",
      "Epoch 162/256\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.3896 - accuracy: 0.8447 - val_loss: 0.3425 - val_accuracy: 0.8889\n",
      "Epoch 163/256\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3892 - accuracy: 0.8458 - val_loss: 0.3418 - val_accuracy: 0.8889\n",
      "Epoch 164/256\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3892 - accuracy: 0.8447 - val_loss: 0.3416 - val_accuracy: 0.8889\n",
      "Epoch 165/256\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3889 - accuracy: 0.8447 - val_loss: 0.3416 - val_accuracy: 0.8889\n",
      "Epoch 166/256\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3883 - accuracy: 0.8458 - val_loss: 0.3410 - val_accuracy: 0.8889\n",
      "Epoch 167/256\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3879 - accuracy: 0.8447 - val_loss: 0.3381 - val_accuracy: 0.8889\n",
      "Epoch 168/256\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3875 - accuracy: 0.8447 - val_loss: 0.3339 - val_accuracy: 0.8889\n",
      "Epoch 169/256\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3871 - accuracy: 0.8435 - val_loss: 0.3320 - val_accuracy: 0.8889\n",
      "Epoch 170/256\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3869 - accuracy: 0.8447 - val_loss: 0.3370 - val_accuracy: 0.8889\n",
      "Epoch 171/256\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3862 - accuracy: 0.8447 - val_loss: 0.3439 - val_accuracy: 0.8889\n",
      "Epoch 172/256\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.3861 - accuracy: 0.8447 - val_loss: 0.3471 - val_accuracy: 0.8889\n",
      "Epoch 173/256\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.3856 - accuracy: 0.8458 - val_loss: 0.3480 - val_accuracy: 0.8889\n",
      "Epoch 174/256\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3851 - accuracy: 0.8458 - val_loss: 0.3462 - val_accuracy: 0.8889\n",
      "Epoch 175/256\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3848 - accuracy: 0.8458 - val_loss: 0.3414 - val_accuracy: 0.8889\n",
      "Epoch 176/256\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3845 - accuracy: 0.8447 - val_loss: 0.3427 - val_accuracy: 0.8889\n",
      "Epoch 177/256\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3840 - accuracy: 0.8447 - val_loss: 0.3426 - val_accuracy: 0.8889\n",
      "Epoch 178/256\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3835 - accuracy: 0.8458 - val_loss: 0.3482 - val_accuracy: 0.8889\n",
      "Epoch 179/256\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3833 - accuracy: 0.8469 - val_loss: 0.3538 - val_accuracy: 0.8889\n",
      "Epoch 180/256\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3831 - accuracy: 0.8481 - val_loss: 0.3525 - val_accuracy: 0.8889\n",
      "Epoch 181/256\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.3824 - accuracy: 0.8481 - val_loss: 0.3464 - val_accuracy: 0.8889\n",
      "Epoch 182/256\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3820 - accuracy: 0.8481 - val_loss: 0.3449 - val_accuracy: 0.8889\n",
      "Epoch 183/256\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3823 - accuracy: 0.8481 - val_loss: 0.3423 - val_accuracy: 0.8889\n",
      "Epoch 184/256\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3819 - accuracy: 0.8469 - val_loss: 0.3483 - val_accuracy: 0.8889\n",
      "Epoch 185/256\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3812 - accuracy: 0.8492 - val_loss: 0.3524 - val_accuracy: 0.8889\n",
      "Epoch 186/256\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3806 - accuracy: 0.8492 - val_loss: 0.3551 - val_accuracy: 0.8889\n",
      "Epoch 187/256\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.3805 - accuracy: 0.8492 - val_loss: 0.3478 - val_accuracy: 0.8889\n",
      "Epoch 188/256\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3803 - accuracy: 0.8469 - val_loss: 0.3453 - val_accuracy: 0.8889\n",
      "Epoch 189/256\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3799 - accuracy: 0.8492 - val_loss: 0.3483 - val_accuracy: 0.8889\n",
      "Epoch 190/256\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.3794 - accuracy: 0.8481 - val_loss: 0.3505 - val_accuracy: 0.8889\n",
      "Epoch 191/256\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3790 - accuracy: 0.8469 - val_loss: 0.3524 - val_accuracy: 0.8889\n",
      "Epoch 192/256\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3787 - accuracy: 0.8492 - val_loss: 0.3511 - val_accuracy: 0.8889\n",
      "Epoch 193/256\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3786 - accuracy: 0.8481 - val_loss: 0.3522 - val_accuracy: 0.8889\n",
      "Epoch 194/256\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3778 - accuracy: 0.8492 - val_loss: 0.3506 - val_accuracy: 0.8889\n",
      "Epoch 195/256\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3779 - accuracy: 0.8481 - val_loss: 0.3530 - val_accuracy: 0.8889\n",
      "Epoch 196/256\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3774 - accuracy: 0.8481 - val_loss: 0.3519 - val_accuracy: 0.8889\n",
      "Epoch 197/256\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3772 - accuracy: 0.8503 - val_loss: 0.3529 - val_accuracy: 0.8889\n",
      "Epoch 198/256\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3767 - accuracy: 0.8503 - val_loss: 0.3578 - val_accuracy: 0.8889\n",
      "Epoch 199/256\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3762 - accuracy: 0.8492 - val_loss: 0.3604 - val_accuracy: 0.8889\n",
      "Epoch 200/256\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3759 - accuracy: 0.8503 - val_loss: 0.3548 - val_accuracy: 0.8889\n",
      "Epoch 201/256\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3756 - accuracy: 0.8515 - val_loss: 0.3459 - val_accuracy: 0.8889\n",
      "Epoch 202/256\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3752 - accuracy: 0.8481 - val_loss: 0.3474 - val_accuracy: 0.8889\n",
      "Epoch 203/256\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3747 - accuracy: 0.8492 - val_loss: 0.3542 - val_accuracy: 0.8889\n",
      "Epoch 204/256\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3755 - accuracy: 0.8503 - val_loss: 0.3578 - val_accuracy: 0.8889\n",
      "Epoch 205/256\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3742 - accuracy: 0.8492 - val_loss: 0.3565 - val_accuracy: 0.8889\n",
      "Epoch 206/256\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3742 - accuracy: 0.8492 - val_loss: 0.3554 - val_accuracy: 0.8889\n",
      "Epoch 207/256\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3738 - accuracy: 0.8492 - val_loss: 0.3511 - val_accuracy: 0.8889\n",
      "Epoch 208/256\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.3735 - accuracy: 0.8492 - val_loss: 0.3560 - val_accuracy: 0.8889\n",
      "Epoch 209/256\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3727 - accuracy: 0.8492 - val_loss: 0.3512 - val_accuracy: 0.8889\n",
      "Epoch 210/256\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3727 - accuracy: 0.8492 - val_loss: 0.3546 - val_accuracy: 0.8889\n",
      "Epoch 211/256\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3722 - accuracy: 0.8492 - val_loss: 0.3520 - val_accuracy: 0.8889\n",
      "Epoch 212/256\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3720 - accuracy: 0.8503 - val_loss: 0.3567 - val_accuracy: 0.8889\n",
      "Epoch 213/256\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3716 - accuracy: 0.8503 - val_loss: 0.3538 - val_accuracy: 0.8889\n",
      "Epoch 214/256\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3709 - accuracy: 0.8492 - val_loss: 0.3508 - val_accuracy: 0.8889\n",
      "Epoch 215/256\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3707 - accuracy: 0.8492 - val_loss: 0.3517 - val_accuracy: 0.8889\n",
      "Epoch 216/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3703 - accuracy: 0.8492 - val_loss: 0.3541 - val_accuracy: 0.8889\n",
      "Epoch 217/256\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3705 - accuracy: 0.8503 - val_loss: 0.3643 - val_accuracy: 0.8889\n",
      "Epoch 218/256\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3697 - accuracy: 0.8503 - val_loss: 0.3660 - val_accuracy: 0.8889\n",
      "Epoch 219/256\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3694 - accuracy: 0.8503 - val_loss: 0.3630 - val_accuracy: 0.8889\n",
      "Epoch 220/256\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3691 - accuracy: 0.8503 - val_loss: 0.3650 - val_accuracy: 0.8889\n",
      "Epoch 221/256\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3687 - accuracy: 0.8526 - val_loss: 0.3574 - val_accuracy: 0.8889\n",
      "Epoch 222/256\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.3686 - accuracy: 0.8503 - val_loss: 0.3532 - val_accuracy: 0.8889\n",
      "Epoch 223/256\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3682 - accuracy: 0.8503 - val_loss: 0.3517 - val_accuracy: 0.8889\n",
      "Epoch 224/256\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3680 - accuracy: 0.8503 - val_loss: 0.3610 - val_accuracy: 0.8889\n",
      "Epoch 225/256\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3678 - accuracy: 0.8515 - val_loss: 0.3729 - val_accuracy: 0.8889\n",
      "Epoch 226/256\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.3677 - accuracy: 0.8526 - val_loss: 0.3750 - val_accuracy: 0.8889\n",
      "Epoch 227/256\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3670 - accuracy: 0.8526 - val_loss: 0.3626 - val_accuracy: 0.8889\n",
      "Epoch 228/256\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.3672 - accuracy: 0.8503 - val_loss: 0.3545 - val_accuracy: 0.8889\n",
      "Epoch 229/256\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3663 - accuracy: 0.8526 - val_loss: 0.3542 - val_accuracy: 0.8889\n",
      "Epoch 230/256\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3662 - accuracy: 0.8526 - val_loss: 0.3580 - val_accuracy: 0.8889\n",
      "Epoch 231/256\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3654 - accuracy: 0.8503 - val_loss: 0.3682 - val_accuracy: 0.8889\n",
      "Epoch 232/256\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3658 - accuracy: 0.8526 - val_loss: 0.3728 - val_accuracy: 0.8889\n",
      "Epoch 233/256\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3661 - accuracy: 0.8537 - val_loss: 0.3593 - val_accuracy: 0.8889\n",
      "Epoch 234/256\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3648 - accuracy: 0.8537 - val_loss: 0.3658 - val_accuracy: 0.8889\n",
      "Epoch 235/256\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3643 - accuracy: 0.8526 - val_loss: 0.3707 - val_accuracy: 0.8889\n",
      "Epoch 236/256\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3638 - accuracy: 0.8526 - val_loss: 0.3721 - val_accuracy: 0.8889\n",
      "Epoch 237/256\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3636 - accuracy: 0.8560 - val_loss: 0.3660 - val_accuracy: 0.8889\n",
      "Epoch 238/256\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3636 - accuracy: 0.8560 - val_loss: 0.3580 - val_accuracy: 0.8889\n",
      "Epoch 239/256\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3630 - accuracy: 0.8549 - val_loss: 0.3607 - val_accuracy: 0.8889\n",
      "Epoch 240/256\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.3630 - accuracy: 0.8549 - val_loss: 0.3626 - val_accuracy: 0.8889\n",
      "Epoch 241/256\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3624 - accuracy: 0.8549 - val_loss: 0.3552 - val_accuracy: 0.8889\n",
      "Epoch 242/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3621 - accuracy: 0.8537 - val_loss: 0.3584 - val_accuracy: 0.8889\n",
      "Epoch 243/256\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3622 - accuracy: 0.8560 - val_loss: 0.3599 - val_accuracy: 0.8889\n",
      "Epoch 244/256\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.3615 - accuracy: 0.8571 - val_loss: 0.3557 - val_accuracy: 0.8889\n",
      "Epoch 245/256\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3618 - accuracy: 0.8560 - val_loss: 0.3513 - val_accuracy: 0.8889\n",
      "Epoch 246/256\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.3613 - accuracy: 0.8560 - val_loss: 0.3601 - val_accuracy: 0.8889\n",
      "Epoch 247/256\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3607 - accuracy: 0.8571 - val_loss: 0.3617 - val_accuracy: 0.8889\n",
      "Epoch 248/256\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3605 - accuracy: 0.8571 - val_loss: 0.3603 - val_accuracy: 0.8889\n",
      "Epoch 249/256\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3601 - accuracy: 0.8571 - val_loss: 0.3669 - val_accuracy: 0.8889\n",
      "Epoch 250/256\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3597 - accuracy: 0.8571 - val_loss: 0.3608 - val_accuracy: 0.8889\n",
      "Epoch 251/256\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3600 - accuracy: 0.8549 - val_loss: 0.3580 - val_accuracy: 0.8889\n",
      "Epoch 252/256\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3590 - accuracy: 0.8560 - val_loss: 0.3572 - val_accuracy: 0.8889\n",
      "Epoch 253/256\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3593 - accuracy: 0.8571 - val_loss: 0.3551 - val_accuracy: 0.8889\n",
      "Epoch 254/256\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3589 - accuracy: 0.8571 - val_loss: 0.3641 - val_accuracy: 0.8889\n",
      "Epoch 255/256\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3585 - accuracy: 0.8583 - val_loss: 0.3646 - val_accuracy: 0.8889\n",
      "Epoch 256/256\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3585 - accuracy: 0.8583 - val_loss: 0.3655 - val_accuracy: 0.8889\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 - 0s - loss: 0.3576 - accuracy: 0.8586 - 34ms/epoch - 1ms/step\n",
      "Accuracy: 0.8585858345031738\n"
     ]
    }
   ],
   "source": [
    " # 3. 训练\n",
    "model = train(train_dataset, labels, epochs=256, is_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.真实预测并输出csv predict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 真实预测并输出csv\n",
    "def predict_out(model, csv_path):\n",
    "    # model.evaluate 和 model.predict 的区别\n",
    "    # https://blog.csdn.net/DoReAGON/article/details/88552348\n",
    "    # 两者差异：\n",
    "    # 1\n",
    "    # 输入输出不同\n",
    "    # model.evaluate输入数据(data)和金标准(label),然后将预测结果与金标准相比较,得到两者误差并输出.\n",
    "    # model.predict输入数据(data),输出预测结果\n",
    "    # 2\n",
    "    # 是否需要真实标签(金标准)\n",
    "    # model.evaluate需要,因为需要比较预测结果与真实标签的误差\n",
    "    # model.predict不需要,只是单纯输出预测结果,全程不需要金标准的参与.\n",
    "    predictions = model.predict(test_dataset)\n",
    "    # 通过astype()方法可以强制转换数据的类型。\n",
    "    predictions = (tf.sigmoid(predictions).numpy().flatten() > 0.5).astype(int)\n",
    "    print(predictions.shape, predictions)\n",
    "    # 输出结果\n",
    "    output = pd.DataFrame({\"PassengerId\": passenger_id, \"Survived\": predictions})\n",
    "    # index=False 不保存行索引,index=是否保留行索引\n",
    "    output.to_csv(csv_path, index=False)\n",
    "    print(f\"您的提交文件保存成功! 位置在{csv_path}\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取年月日时分秒 get_time\n",
    "\n",
    "在输出预测结果csv的时候附上时间，方便区别。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取年月日时分秒\n",
    "def get_time():\n",
    "    return datetime.datetime.now().strftime(\"%Y%m%d%H%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 925us/step\n",
      "(418,) [0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 0\n",
      " 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0\n",
      " 1 0 0 1 0 1 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 1 0 1\n",
      " 0 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0\n",
      " 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0\n",
      " 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0\n",
      " 0 1 1 1 1 0 0 1 0 0 0]\n",
      "您的提交文件保存成功! 位置在./submission_202208181521.csv\n"
     ]
    }
   ],
   "source": [
    "# 获取预处理后的测试集和序号\n",
    "test_dataset, passenger_id = preprocess(\n",
    "    raw_test_dataset, features_test, train=False\n",
    ")\n",
    "\n",
    "csv_path = f\"./submission_{get_time()}.csv\"\n",
    "prediction = predict_out(model, csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 导入数据\n",
    "path_url = r\"kaggle\\titanic\\train.csv\"\n",
    "test_path_url = r\"kaggle\\titanic\\test.csv\"\n",
    "\n",
    "raw_train_dataset, raw_test_dataset = load_data(path_url, test_path_url)\n",
    "\n",
    "# 2. 数据预处理\n",
    "features_test = [\n",
    "    \"PassengerId\",\n",
    "    \"Pclass\",\n",
    "    \"Sex\",\n",
    "    \"Fare\",\n",
    "    \"Age\",\n",
    "    \"SibSp\",\n",
    "    \"Parch\",\n",
    "    \"Embarked\",\n",
    "]\n",
    "features_train = features_test + [\"Survived\"]\n",
    "\n",
    "# 获取预处理后的训练集和标签\n",
    "train_dataset, labels = preprocess(raw_train_dataset, features_train)\n",
    "\n",
    "# 3. 训练\n",
    "model = train(train_dataset, labels, epochs=256, is_plot=True)\n",
    "\n",
    "# 获取预处理后的测试集和序号\n",
    "test_dataset, passenger_id = preprocess(\n",
    "    raw_test_dataset, features_test, train=False\n",
    ")\n",
    "\n",
    "csv_path = f\"./submission_{get_time()}.csv\"\n",
    "prediction = predict_out(model, csv_path)\n",
    "\n",
    "# 验证与原始数据raw_test_dataset长度是否一致\n",
    "if prediction.shape[0] == raw_test_dataset.shape[0]:\n",
    "    print(f\"--预测长度={prediction.shape[0]}校验通过 √\")\n",
    "else:\n",
    "    print(\n",
    "        f\"--预测长度与raw_test_dataset长度不一致 ×,prediction.shape={prediction.shape},raw_test_dataset.shape={raw_test_dataset.shape}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('app')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "645379302be42ca707196664ab2ea5b5671642deba7f708676c22024debb4514"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
